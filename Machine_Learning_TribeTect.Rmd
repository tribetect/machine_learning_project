---
title: Hey! Do you even lift (correctly) bro? OR Predicting correct manner of lifting
  a dumbell using wearable gadgets
author: "TribeTect"
output: html_document
---
## Introduction
Devices such as Jawbone Up, Nike FuelBand, and Fitbit help collect large amounts of personal activity data. Beyond finding patterns in behavior, these devices can potentially help improve the form and manner of activity. These applications of correcting activity have tremendous potential in preventing injury or conducting forensics on activity-based events.

```{r initialize, echo=FALSE, warning=FALSE, message=FALSE}
setwd("E:/Dropbox/Coursera/8. Machine Learning/project")

set.seed(22398)

invisible(lapply(list("caret", "ggplot2", "rpart", "rattle"), require, character.only=TRUE))

training_supplied <- read.csv("pml-training.csv")
testing_supplied <- read.csv("pml-testing.csv")
```


## Objective 
The goal of your project is to reliably predict the manner in which test subjects used a dumbell for exercise. There is one right way (Class A) and four incorrect manners (B - E)

## About the Dataset
Data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, asked to perform barbell lifts correctly and incorrectly, in 5 different ways. 

These healthy participants performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

The model needs to predict the manner of exercise, given by the "classe" variable in the training set. There are **160 variables** in all and 19622 **data-points (rows)**.

The **test data with 20 cases** without the classe variabe serves as the final prediction performance test for the model.

## Model Building
To build a prediction model, choices were made to ensure an elegant model, selecting variables most-likely to contribute individually to the model, selecting a model, cross validating it. 

To build the model for predicting manner of exercise being performed, we rejected variables least related to exercise class. 

The caret package was used for model training and improvement.


### Choices made to approach the problem
Amongst the 159 variables, choices were made to eliminate variables likely unrelated to the exercise class variables.

* Variables with mostly "NA" data were eliminated with visual inspection of the training data as a table
* Individual components (x,y,z) are ignored in favor of **total_** type variables
* Variables measured in degrees were initially ignored 
* Arm and forearm are common visual predictors of dumbell exercise manner per student's expert judgement  

Initial predictor set: Total arm acceleration, total dumbell acceleration, total belt acceleration

```{r echo=FALSE}
selected_variables <- c("total_accel_arm", "total_accel_belt", "roll_forearm", "pitch_forearm", "yaw_forearm", "roll_arm", "pitch_arm", "yaw_arm","gyros_forearm_z", "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell")

training_supplied <- training_supplied[,c("classe",selected_variables)]
testing_supplied <- testing_supplied[,selected_variables]

```


### Cross Validation
To validate the model before trying it on the supplied test data the supplied training data set was split into training and testing sets, in the ratio 7:3. 

```{r load data, echo=TRUE, message=FALSE}
inTrain <- createDataPartition(y=training_supplied$classe, p=0.7, list=FALSE)
training <- training_supplied[inTrain,]
testing <- training_supplied[-inTrain,]
```

### Preprocessing
As we trained the model preprocessing for scale and center to normalize the data did not increase the model accuracy significantly. So tree-based prediction was used **without preprocessing.**

### Model training with Trees
```{r model training}
modFit <- train(classe ~., data=training, method="rpart") # Trees (accuracy in 0.30s)
#modFit$finalModel

fancyRpartPlot(modFit$finalModel)
modFit$finalModel
```

### Expected out of sample error rate

This is where we examine the fitness of the model to the data that was not used to train the model. The confusion matrix indicates the performance of the model internally.

Comparing predictions for the 30% data set aside with actual classifications, will reveal the efficacy of this model.

```{r confusion matrix}
  predicted_manner <- predict(modFit, newdata=testing)
  #summary(predicted_manner)
  confusionMatrix(testing$classe, predicted_manner)
```


### Prediction performance on the 20 test cases

```{r predicting on test cases}
  predict(modFit, newdata=testing_supplied)
```

### Prediction using cross-validation

Using cross-validation training method K Nearest Neighbors **DOUBLES** model performance and scored **100% accurate on the quiz** associated with the project 

```{r CV based training}

  modFitCV <- train(classe ~., data=training, method = "knn")
  #modFitCV$finalModel
  predictCV <- predict(modFitCV, newdata=testing)
  confusionMatrix(predictCV,testing$classe)
```
### Prediction performance on the 20 test cases

```{r predicting on test cases WITH cv}
  predict(modFitCV, newdata=testing_supplied)
```

## Conclusion

Predicting if the manner of exercise was correct or incorrect required both instinct and experimentations through validation modeling. The tree-based modeling gave the opportunity to fine tune the variables, initially selected through instinct and common knowledge on what makes a dumbell exercise right or wrong.

The use of metrics in comparing relative performance of models is invaluable. K-folding technique revealed a far better prediction model compared to the popular random forest method. 

## References
More information is available from the website here: 
[http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).] (http://groupware.les.inf.puc-rio.br/har)

## Acknowledgements

[The data for this project comes from Catholic Public University in Rio de Janeiro (PUC) More information at http://groupware.les.inf.puc-rio.br/har] (http://groupware.les.inf.puc-rio.br/har)